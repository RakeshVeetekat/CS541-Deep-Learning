{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 Output:\n",
      "train/validation unregularized MSE:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rakes\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\rakes\\AppData\\Local\\Temp\\ipykernel_11520\\3979377061.py:36: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - (learn_rate * d_of_w)\n",
      "C:\\Users\\rakes\\AppData\\Local\\Temp\\ipykernel_11520\\3979377061.py:37: RuntimeWarning: invalid value encountered in subtract\n",
      "  b = b - (learn_rate * d_of_b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  170.21064791764127\n",
      "train/validation unregularized MSE:  135.47496519515798\n",
      "train/validation unregularized MSE:  150.15603253858993\n",
      "train/validation unregularized MSE:  144.32203983141477\n",
      "train/validation unregularized MSE:  136.35303705553747\n",
      "train/validation unregularized MSE:  136.89091361137616\n",
      "train/validation unregularized MSE:  137.40048043523007\n",
      "train/validation unregularized MSE:  134.83801409750794\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  137.1889942009483\n",
      "train/validation unregularized MSE:  147.15633217719605\n",
      "train/validation unregularized MSE:  133.48721249704104\n",
      "train/validation unregularized MSE:  135.72377069380403\n",
      "train/validation unregularized MSE:  136.49086007970536\n",
      "train/validation unregularized MSE:  136.12489959989404\n",
      "train/validation unregularized MSE:  135.82999072190552\n",
      "train/validation unregularized MSE:  135.80759045601127\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\AppData\\Local\\Temp\\ipykernel_11520\\3979377061.py:10: RuntimeWarning: overflow encountered in square\n",
      "  return (1/(2 * a)) * np.sum(np.square(y_pred - y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  135.59989915424987\n",
      "train/validation unregularized MSE:  135.38186982678764\n",
      "train/validation unregularized MSE:  135.21050328093162\n",
      "train/validation unregularized MSE:  135.74104086290356\n",
      "train/validation unregularized MSE:  137.22904685927196\n",
      "train/validation unregularized MSE:  136.1779473062348\n",
      "train/validation unregularized MSE:  136.56620118664006\n",
      "train/validation unregularized MSE:  136.90164715080323\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  8.752355009211058e+194\n",
      "train/validation unregularized MSE:  7.300769064454156e+194\n",
      "train/validation unregularized MSE:  7.94516611004122e+194\n",
      "train/validation unregularized MSE:  8.312149399119605e+194\n",
      "train/validation unregularized MSE:  136.12378814775857\n",
      "train/validation unregularized MSE:  135.47968511186653\n",
      "train/validation unregularized MSE:  135.64613975138806\n",
      "train/validation unregularized MSE:  135.45422390889826\n",
      "train/validation unregularized MSE:  136.16998735741439\n",
      "train/validation unregularized MSE:  136.29357200099545\n",
      "train/validation unregularized MSE:  136.47316249859082\n",
      "train/validation unregularized MSE:  136.15550238088403\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  133.92381277137423\n",
      "train/validation unregularized MSE:  147.47559982474502\n",
      "train/validation unregularized MSE:  135.93511482822754\n",
      "train/validation unregularized MSE:  152.69129790655583\n",
      "train/validation unregularized MSE:  133.9195372038084\n",
      "train/validation unregularized MSE:  134.37130363815083\n",
      "train/validation unregularized MSE:  135.38429598578955\n",
      "train/validation unregularized MSE:  135.77065957242195\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  134.92615134781076\n",
      "train/validation unregularized MSE:  132.49832879195498\n",
      "train/validation unregularized MSE:  144.82989083349224\n",
      "train/validation unregularized MSE:  134.69786031123263\n",
      "train/validation unregularized MSE:  135.86181118116647\n",
      "train/validation unregularized MSE:  135.7498864444108\n",
      "train/validation unregularized MSE:  134.4781895597993\n",
      "train/validation unregularized MSE:  135.9777643322185\n",
      "train/validation unregularized MSE:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rakes\\AppData\\Local\\Temp\\ipykernel_11520\\3979377061.py:28: RuntimeWarning: overflow encountered in add\n",
      "  y_pred = x.dot(w) + b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  140.804908951711\n",
      "train/validation unregularized MSE:  136.34494844656788\n",
      "train/validation unregularized MSE:  135.1280745065044\n",
      "train/validation unregularized MSE:  134.16497676651386\n",
      "train/validation unregularized MSE:  136.4499166115252\n",
      "train/validation unregularized MSE:  136.04411424209536\n",
      "train/validation unregularized MSE:  136.27094595207345\n",
      "train/validation unregularized MSE:  135.5147804150093\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  inf\n",
      "train/validation unregularized MSE:  134.779622656524\n",
      "train/validation unregularized MSE:  134.98979304035092\n",
      "train/validation unregularized MSE:  133.74739702730355\n",
      "train/validation unregularized MSE:  134.10154151546644\n",
      "train/validation unregularized MSE:  136.94756522456964\n",
      "train/validation unregularized MSE:  137.13128812204997\n",
      "train/validation unregularized MSE:  136.12429545868616\n",
      "train/validation unregularized MSE:  136.97662006236143\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  130.53625688633375\n",
      "train/validation unregularized MSE:  129.75050048964476\n",
      "train/validation unregularized MSE:  154.08044463941076\n",
      "train/validation unregularized MSE:  133.36050261846475\n",
      "train/validation unregularized MSE:  135.25848397683106\n",
      "train/validation unregularized MSE:  133.5039486551229\n",
      "train/validation unregularized MSE:  134.0113843516638\n",
      "train/validation unregularized MSE:  134.57761664290004\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  131.06677406447113\n",
      "train/validation unregularized MSE:  135.4909948104081\n",
      "train/validation unregularized MSE:  141.96174458495932\n",
      "train/validation unregularized MSE:  145.7917645609694\n",
      "train/validation unregularized MSE:  136.58092989720325\n",
      "train/validation unregularized MSE:  135.66351684090725\n",
      "train/validation unregularized MSE:  134.7150429282501\n",
      "train/validation unregularized MSE:  135.2281931603368\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  132.6614593410059\n",
      "train/validation unregularized MSE:  134.6296907456717\n",
      "train/validation unregularized MSE:  137.87658069446098\n",
      "train/validation unregularized MSE:  134.82823754916683\n",
      "train/validation unregularized MSE:  135.28296979197893\n",
      "train/validation unregularized MSE:  136.2710570003861\n",
      "train/validation unregularized MSE:  135.82295170775353\n",
      "train/validation unregularized MSE:  136.67224377179915\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  135.34874356748114\n",
      "train/validation unregularized MSE:  134.7489809490512\n",
      "train/validation unregularized MSE:  134.48031380552717\n",
      "train/validation unregularized MSE:  133.9500058447134\n",
      "train/validation unregularized MSE:  136.09522081357562\n",
      "train/validation unregularized MSE:  136.76843512676024\n",
      "train/validation unregularized MSE:  136.66260649383534\n",
      "train/validation unregularized MSE:  136.86499762023251\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  136.36327304526927\n",
      "train/validation unregularized MSE:  135.7564715276333\n",
      "train/validation unregularized MSE:  145.9410073456845\n",
      "train/validation unregularized MSE:  126.77897172963536\n",
      "train/validation unregularized MSE:  133.1187477090769\n",
      "train/validation unregularized MSE:  135.98827572568706\n",
      "train/validation unregularized MSE:  134.441325444631\n",
      "train/validation unregularized MSE:  134.62321373502994\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  145.00975498709258\n",
      "train/validation unregularized MSE:  131.65551295581588\n",
      "train/validation unregularized MSE:  138.07213094585333\n",
      "train/validation unregularized MSE:  132.40204116730118\n",
      "train/validation unregularized MSE:  135.42505301291655\n",
      "train/validation unregularized MSE:  134.99441770355858\n",
      "train/validation unregularized MSE:  135.3449843332285\n",
      "train/validation unregularized MSE:  135.49078806498025\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  133.81165885225005\n",
      "train/validation unregularized MSE:  133.06975983411286\n",
      "train/validation unregularized MSE:  133.68251944154463\n",
      "train/validation unregularized MSE:  132.94202141239637\n",
      "train/validation unregularized MSE:  135.8554379164131\n",
      "train/validation unregularized MSE:  136.24333632551355\n",
      "train/validation unregularized MSE:  136.1777849335946\n",
      "train/validation unregularized MSE:  135.78908971926828\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  nan\n",
      "train/validation unregularized MSE:  135.37394869615466\n",
      "train/validation unregularized MSE:  135.9969684254965\n",
      "train/validation unregularized MSE:  137.21319046108732\n",
      "train/validation unregularized MSE:  134.94265993858565\n",
      "train/validation unregularized MSE:  135.7630489204891\n",
      "train/validation unregularized MSE:  135.75219972793278\n",
      "train/validation unregularized MSE:  135.44818788652464\n",
      "train/validation unregularized MSE:  135.8371500736761\n",
      "\n",
      "\n",
      "Results of training:\n",
      "best error from validation dataset:  126.77897172963536\n",
      "best learning rate:  0.001\n",
      "best number of epochs:  20\n",
      "best batch size:  10\n",
      "best reg term:  0.1\n",
      "unregularized MSE from test dataset:  139.17092912962255\n"
     ]
    }
   ],
   "source": [
    "# Problem 3\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate MSE given y_pred and y\n",
    "def MSE(y_pred, y):\n",
    "\ta = y.shape[0]\n",
    "\treturn (1/(2 * a)) * np.sum(np.square(y_pred - y))\n",
    "\n",
    "# Calculate the MSE given the prediction parameters and actual y value\n",
    "def calculate_error(x, w, b, y):\n",
    "\ty_pred = x.dot(w) + b\n",
    "\treturn MSE(y_pred, y)\n",
    "\n",
    "# Loop through the samples based on the batch_size hyperparameter\n",
    "def batch_loop(x, y, size):\n",
    "\tif(len(x) == len(y)):\n",
    "\t\trandom_x = x[np.random.permutation(x.shape[0])]\n",
    "\t\trandom_y = y[np.random.permutation(y.shape[0])]\n",
    "\t\tfor i in np.arange(0, y.shape[0], size):\n",
    "\t\t\tyield random_x[i:i + size], random_y[i:i + size]\n",
    "\n",
    "# Perform gradient_descent using the prediction parameters, actual y, learning rate, and regularized term\n",
    "def gradient_descent(x, y, b, w, learn_rate, alpha = 0.1):\n",
    "\ta = y.shape[0]\n",
    "\ty_pred = x.dot(w) + b\n",
    "\n",
    "\t# Store the derivatives of w and b\n",
    "\td_of_w = (1/a) * x.T.dot(y_pred - y)\n",
    "\td_of_b = (1/a) * np.sum(y_pred - y)\n",
    "\t# Add the regularized term to the derivative of w\n",
    "\td_of_w += ((alpha/a) * w)\n",
    "\t# Update parameters with gradient descent\n",
    "\tw = w - (learn_rate * d_of_w)\n",
    "\tb = b - (learn_rate * d_of_b)\n",
    "\n",
    "\treturn w, b\n",
    "\n",
    "# Perform SGD\n",
    "def stochastic_gradient_descent(x, y, b, w, learn_rate: float, num_of_epoch: int, batch_size: int, dataset_size: int, alpha):\n",
    "\t\n",
    "\tfor n in range(num_of_epoch - 1):\n",
    "\t\tfor mini_batch_x, mini_batch_y in batch_loop(x, y, batch_size):\n",
    "\t\t\tw, b = gradient_descent(mini_batch_x, mini_batch_y, b, w, learn_rate, alpha)\n",
    "\t\t\n",
    "\treturn w, b \n",
    "\n",
    "# Find the lowest error by performing SGD\n",
    "def find_lowest_error(x, y, learn_rate, num_of_epoch, batch_size, alpha):\n",
    "\tdataset_size = len(y)\n",
    "\tm = np.expand_dims(a=y, axis=-1)\n",
    "\tw = np.random.rand(x.shape[1]) * np.sqrt(1/(x.shape[1] + m.shape[1]))\n",
    "\tb = np.random.rand(m.shape[1])\n",
    "\tw_trained, b_trained = stochastic_gradient_descent(x, y, b, w, learn_rate, num_of_epoch, batch_size, dataset_size, alpha)\n",
    "\treturn w_trained, b_trained\n",
    "\n",
    "# Implementation of grid_search to tune our hyperparameters by looping through the various values we have for each\n",
    "def grid_search():\n",
    "\thyperparameters = {\n",
    "\t\t\"learn_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "\t\t\"num_of_epoch\": [5, 10, 15, 20],\n",
    "\t\t\"batch_size\": [10, 20, 50, 100],\n",
    "\t\t\"alpha\": [0.75, 0.5, 0.25, 0.1]\t\n",
    "\t}\n",
    "\tfor a in range(len(hyperparameters[\"num_of_epoch\"])):\n",
    "\t\tfor b in range(len(hyperparameters[\"batch_size\"])):\n",
    "\t\t\tfor c in range(len(hyperparameters[\"learn_rate\"])):\n",
    "\t\t\t\tfor d in range(len(hyperparameters[\"alpha\"])):\n",
    "\t\t\t\t\t\tyield hyperparameters[\"num_of_epoch\"][a], hyperparameters[\"batch_size\"][b], hyperparameters[\"learn_rate\"][c], hyperparameters[\"alpha\"][d]\n",
    "\n",
    "# This function will train the age regressor based on various hyperparameters provided in the grid_search() function\n",
    "def train_age_regressor():\n",
    "\t\n",
    "\t# Load data\n",
    "\tstarting_x_tr = np.reshape(np.load(\"age_regression_Xtr.npy\"), (-1, 48*48))\n",
    "\tstarting_y_tr = np.load(\"age_regression_ytr.npy\")\n",
    "\tx_te = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))\n",
    "\ty_te = np.load(\"age_regression_yte.npy\")\n",
    "\tx_tr, x_val, y_tr, y_val = train_test_split(starting_x_tr, starting_y_tr, train_size=0.8)\n",
    "\n",
    "\t# Initialize best hyperparameter values as the worst they could be\n",
    "\tbest_error = 1000000\n",
    "\tbest_num_of_epoch = -1\n",
    "\tbest_batch_size = -1\n",
    "\tbest_learn_rate = -1\n",
    "\tbest_alpha = -1\n",
    "\n",
    "\t# Loop through each combination of the hyperparameters in grid_search() to find the best combination to minimize MSE\n",
    "\tfor num_of_epoch, batch_size, learn_rate, alpha in grid_search():\n",
    "\n",
    "\t\tw_trained, b_trained = find_lowest_error(x_tr, y_tr, learn_rate, num_of_epoch, batch_size, alpha)\n",
    "\n",
    "\t\t# Calculate the MSE from the validation dataset\n",
    "\t\terror = calculate_error(x_val, w_trained, b_trained, y_val)\n",
    "\t\tprint(\"train/validation unregularized MSE: \", error)\n",
    "\t\tprint(x_val.shape)\n",
    "\n",
    "\t\t# Store the hyperparameters that led to reduced error in the following variables\n",
    "\t\tif error < best_error:\n",
    "\t\t\tbest_error = error\n",
    "\t\t\tbest_learn_rate = learn_rate\n",
    "\t\t\tbest_num_of_epoch = num_of_epoch\n",
    "\t\t\tbest_batch_size = batch_size\n",
    "\t\t\tbest_alpha = alpha\n",
    "\n",
    "\t# Finally, calculate the error using the trained weights and biases\n",
    "\terror = calculate_error(x_te, w_trained, b_trained, y_te)\n",
    "\tprint(\"\\n\")\n",
    "\tprint(\"Results of training:\")\n",
    "\tprint(\"best error from validation dataset: \", best_error)\n",
    "\tprint(\"best learning rate: \", best_learn_rate)\n",
    "\tprint(\"best number of epochs: \", best_num_of_epoch)\n",
    "\tprint(\"best batch size: \", best_batch_size)\n",
    "\tprint(\"best reg term: \", best_alpha)\n",
    "\tprint(\"unregularized MSE from test dataset: \", error)\n",
    "\n",
    "\treturn w_trained, b_trained\n",
    "\n",
    "def main():\n",
    "\n",
    "\tprint(\"Problem 3 Output:\")\n",
    "\tw_output, b_output = train_age_regressor()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 4c Output:\n",
      "-1.0\n",
      "[ -8 -10]\n"
     ]
    }
   ],
   "source": [
    "# Problem 4c\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def update_b_mae(X, y, w, b):\n",
    "\n",
    "  # Calculate the predictions.\n",
    "  predictions = X.dot(w) + b\n",
    "\n",
    "  # Calculate the signs of the errors.\n",
    "  errors = np.sign(predictions - y)\n",
    "\n",
    "  # Update the bias term.\n",
    "  b -= np.mean(errors)\n",
    "\n",
    "  return b\n",
    "\n",
    "def update_w_mae(X, y, w, b):\n",
    "\n",
    "  # Calculate the predictions\n",
    "  y_pred = X.dot(w) + b\n",
    "\n",
    "  # Calculate the gradients\n",
    "  grad_w = X.T.dot(np.sign(y_pred - y))\n",
    "\n",
    "  # Update the weight vector\n",
    "  w_new = w - grad_w\n",
    "\n",
    "  return w_new\n",
    "\n",
    "# Test both functions with initial values for X, y, w, and b\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([3, 7, 11])\n",
    "w = np.array([1, 2])\n",
    "b = 0\n",
    "# Update both parameters.\n",
    "b_new = update_b_mae(X, y, w, b)\n",
    "w_new = update_w_mae(X, y, w, b)\n",
    "print(\"Problem 4c Output:\")\n",
    "print(b_new)\n",
    "print(w_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
