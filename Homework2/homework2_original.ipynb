{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def model(x, w, b):\n",
    "\ty_hat = x.dot(w) + b\n",
    "\treturn y_hat\n",
    "\n",
    "\n",
    "def mean_square_error(y_hat, y):\n",
    "\tn = y.shape[0]\n",
    "\tmse = (1/(2 * n)) * np.sum(np.square(y_hat - y))\n",
    "\treturn mse\n",
    "\n",
    "\n",
    "def l2(w, y, alpha = 0.01):\n",
    "\tn = y.shape[0]\n",
    "\twt_w = (w).dot(w.T) # (w^T)w\n",
    "\treg = (1/(2 * n) * alpha) * wt_w # (alpha/2n) * ((w^T)w) \n",
    "\treturn reg\n",
    "\n",
    "\n",
    "def regularized_mean_square_error(y, y_hat, reg):\n",
    "\tn = y.shape[0]\n",
    "\tmse = mean_square_error(y_hat, y)\n",
    "\treturn mse + reg\n",
    "\n",
    "\n",
    "#Train/Test Split performed by randomly taking inputs and their associated labels and assigning them to either a training group or a test group\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                   |                |\n",
    "#                  Training set                     |   Testing Set  |\n",
    "#                                                   |                |\n",
    "#---------------------------------------------------#----------------#\n",
    "#train_perc should be a value between 0 and 1, eg. train_perc 0.8\n",
    "def random_train_test_split(dataset, train_perc = 0.8):\n",
    "\tperc_of_dataset = dataset.shape[1] * train_perc\n",
    "\tnumpy.random.shuffle(dataset)\n",
    "\n",
    "\ttrain = dataset_arrx[:perc_of_dataset,:] \n",
    "\ttest = dataset_arr[perc_of_dataset:,:]\n",
    "\n",
    "\treturn train, test\n",
    "\n",
    "\n",
    "#Validation/Test Split performed by randomly taking inputs and their associated labels and assigning them to either a validation group or a test group\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                   |                |\n",
    "#                  Training set                     |   Testing Set  |\n",
    "#                                                   |                |\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                             V\n",
    "#                                                |------------#---------#\n",
    "#                                                |            |         |\n",
    "#                                                | Validation | Testing |\n",
    "#                                                |     Set    |   Set   |\n",
    "#                                                |------------#---------#\n",
    "#train_perc should be a value between 0 and 1, eg. train_perc 0.5 for a 50/50 split of the test set \n",
    "def random_test_validation_split(x, y, train_perc = 0.8):\n",
    "\tdataset_row_size, dataset_col_size = x.shape\n",
    "\tvalidation_set_size = int(dataset_row_size * train_perc)\n",
    "\ttest_set_size = int(dataset_row_size - validation_set_size)\n",
    "\n",
    "\t#get random indices for both validation and test sets\n",
    "\tindices = np.random.permutation(x.shape[0])\n",
    "\tvalidation_p_index = indices[:validation_set_size]\n",
    "\ttest_p_index = indices[test_set_size:]\n",
    "\n",
    "\t#validation and test labels\n",
    "\tvalidation_l = y[validation_p_index]\n",
    "\ttest_l = y[test_p_index]\n",
    "\n",
    "\t#validation and test parameters \n",
    "\tvalidation_p = x[validation_p_index]\n",
    "\ttest_p = x[test_p_index]\n",
    "\n",
    "\treturn validation_p, validation_l, test_p, test_l\n",
    "\n",
    "\n",
    "#X is the input \n",
    "#y is the real result\n",
    "#w is the weight \n",
    "#epsilon is the learning rate\n",
    "#alpha is the regularized term\n",
    "#reg_bool if you wish to use the regularized term\n",
    "def gradient_descent(x, y, b, w, learning_rate, reg_bool, alpha = 0.1):\n",
    "\t# batch_size x 2304\n",
    "\tX_t = x.T # 2304 x batch_size\n",
    "\tn = y.shape[0]\n",
    "\ty_hat = model(x, w, b)\n",
    "\n",
    "\t#taking derivative \n",
    "\t# 2304 x 1 \n",
    "\tderivative_of_w = (1/n) * X_t.dot(y_hat - y)\n",
    "\tderivative_of_b = (1/n) * np.sum(y_hat - y)\n",
    "\n",
    "\t# if regularized term is wanted then add that to the derivative with respect to w\n",
    "\tif reg_bool == True:\n",
    "\t\tderivative_of_w += ((alpha/n) * w)\n",
    "\n",
    "\t#this is gradient descent\n",
    "\tw = w - (learning_rate * derivative_of_w)\n",
    "\tb = b - (learning_rate * derivative_of_b)\n",
    "\n",
    "\treturn w, b\n",
    "\n",
    "def batch_iterator(x, y, size_of_batch):\n",
    "\tassert (len(x) == len(y))\n",
    "\tp = np.random.permutation(x.shape[0])\n",
    "\tx_rand = x[p]\n",
    "\ty_rand = y[p]\n",
    "\tfor i in np.arange(0, x.shape[0], size_of_batch):\n",
    "\t\tyield x_rand[i:i + size_of_batch], y_rand[i:i + size_of_batch]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(x, y, b, w, learning_rate: float, num_of_epochs: int, size_of_batch: int, size_of_dataset: int, reg_bool: bool = True, alpha = 0.1):\n",
    "\tpast_cost = 1e100\n",
    "\tcurrent_cost = -1\n",
    "\tepochs_since_improved = 0\n",
    "\t# writer = SummaryWriter()\n",
    "\tcurrent_step = 0\n",
    "\n",
    "\tfor e in range(num_of_epochs - 1):\n",
    "\t\tcurrent_step += 1\n",
    "\t\tfor mini_batch_x, mini_batch_y in batch_iterator(x, y, size_of_batch):\n",
    "\n",
    "\t\t\ty_hat = model(mini_batch_x, w, b)\n",
    "\n",
    "\t\t\tif reg_bool == True:\n",
    "\t\t\t\treg = l2(w, mini_batch_y)\n",
    "\t\t\t\tcurrent_cost = regularized_mean_square_error(mini_batch_y, y_hat, reg)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_cost = mean_square_error(y_hat, mini_batch_y)\n",
    "\t\t\t\t\n",
    "\t\t\tw, b = gradient_descent(mini_batch_x, mini_batch_y, b, w, learning_rate, reg_bool, alpha)\n",
    "\n",
    "\t\tentire_dataset_mse = mean_square_error(model(x, w, b), y)\n",
    "\t\tif math.isnan(entire_dataset_mse):\n",
    "\t\t\tbreak\n",
    "\t\t#print(\"MSE: \", entire_dataset)\n",
    "\t\t\n",
    "\treturn w, b \n",
    "\n",
    "def find_lowest_loss(x, y, learning_rate, num_of_epochs, size_of_batch, reg_bool = True, alpha = 0.1):\n",
    "\tsize_of_dataset = len(y)\n",
    "\tl = np.expand_dims(a=y, axis=-1)\n",
    "\tw_shape = (x.shape[1]) #2304 x 1\n",
    "\tw = np.random.rand(w_shape) * np.sqrt(1/(x.shape[1] + l.shape[1])) #set intial w value based off Xavier intilization\n",
    "\tb = np.random.rand(l.shape[1]) #set initial bias value\n",
    "\treg = l2(w, y) # set regularized term\n",
    "\tw_new, b_new = stochastic_gradient_descent(x, y, b, w, learning_rate, num_of_epochs, size_of_batch, size_of_dataset, reg_bool, alpha)\n",
    "\treturn w_new, b_new\n",
    "\n",
    "\n",
    "def train_valid_test(x, w, b, y, reg: float, reg_bool: bool = False):\n",
    "\ty_hat = model(x, w, b)\n",
    "\tif reg_bool == True:\n",
    "\t\tvalidation_or_test = regularized_mean_square_error(y, y_hat, reg)\n",
    "\telse:\n",
    "\t\tvalidation_or_test = mean_square_error(y_hat, y)\n",
    "\n",
    "\treturn validation_or_test\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\t#training set\n",
    "\tx_tr = np.reshape(np.load(\"age_regression_Xtr.npy\"), (-1, 48*48)) #2304 x 1\n",
    "\ty_tr = np.load(\"age_regression_ytr.npy\")\n",
    "\n",
    "\t#testing set\n",
    "\tx_te = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))\n",
    "\ty_te = np.load(\"age_regression_yte.npy\")\n",
    "\t\n",
    "\treturn x_tr, y_tr, x_te, y_te\n",
    "\n",
    "\n",
    "def load_random_data():\n",
    "\tx = np.random.rand(500, 1)\n",
    "\ty = 4 * x + 3 + np.random.rand(500, 1)\n",
    "\n",
    "\treturn x, y \n",
    "\n",
    "def grid_search():\n",
    "\thyperparameters = {\n",
    "\t\t\"learning_rate\": [0.01, 0.001, 0.0001],\n",
    "\t\t\"num_of_epochs\": [100, 200, 300, 400, 500, 600],\n",
    "\t\t\"size_of_batch\": [1, 10, 50, 100, 200, 5000],\n",
    "\t\t\"alpha\": [0.9, 0.5, 0.1]\t\n",
    "\t}\n",
    "\tfor i in range(len(hyperparameters[\"num_of_epochs\"])):\n",
    "\t\tfor j in range(len(hyperparameters[\"size_of_batch\"])):\n",
    "\t\t\tfor k in range(len(hyperparameters[\"learning_rate\"])):\n",
    "\t\t\t\tfor l in range(len(hyperparameters[\"alpha\"])):\n",
    "\t\t\t\t\t\tyield hyperparameters[\"learning_rate\"][k], hyperparameters[\"num_of_epochs\"][i], hyperparameters[\"size_of_batch\"][j], hyperparameters[\"alpha\"][l]\n",
    "\n",
    "#Function Arguments:\n",
    "#mse_bool is the boolean value that determines if mse will be performed without or with regularization, default is True\n",
    "#alpha is the value for regularization, if used, default is 0.0\n",
    "#ttv_val is the value associated with the type of split wanted. A Train/Test Split is 0, and a train/validation/split is 1, No Split Needed is 2. Default is 2\n",
    "#learning rate is the hyperparameter associated with the gradient descent\n",
    "def train_age_regressor(ttv_val: int = 2, the_set: int = 0, reg_bool = False):\n",
    "\t# Load data\n",
    "\tx_tr, y_tr, x_te, y_te = load_data()\n",
    "\n",
    "\tx_rand, y_rand = load_random_data()\n",
    "\n",
    "\tif ttv_val == 0:\n",
    "\t\tprint(\"I made this before I realized train/test split is already given to you\")\n",
    "\n",
    "\telif ttv_val == 1:\n",
    "\t\tx_val, y_val, x_te, y_te = random_test_validation_split(x_te, y_te, train_perc = 0.8)\n",
    "\n",
    "\t\n",
    "\tbest_loss = 1000000000\n",
    "\tbest_learning_rate = -1\n",
    "\tbest_num_of_epoch = -1\n",
    "\tbest_size_of_batch = -1\n",
    "\tbest_alpha = -1\n",
    "\tfor learning_rate, num_of_epochs, size_of_batch, alpha in grid_search():\n",
    "\n",
    "\t\tw_trained, b_trained = find_lowest_loss(x_tr, y_tr, learning_rate, num_of_epochs, size_of_batch, reg_bool, alpha)\n",
    "\t\treg = l2(w_trained, y_tr)\n",
    "\t\tcheck_loss = train_valid_test(x_te, w_trained, b_trained, y_te, reg, reg_bool)\n",
    "\n",
    "\t\tif math.isnan(check_loss):\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif the_set == 0: #perform test using weights and biases from training set on test set, thus getting loss on test set\n",
    "\t\t\treg = l2(w_trained, y_te)\n",
    "\t\t\tloss = train_valid_test(x_te, w_trained, b_trained, y_te, reg, reg_bool)\n",
    "\t\t\tprint(\"train/test loss: \", loss)\n",
    "\n",
    "\t\telif the_set == 1: #perform test using weights and biases from training set on validation set, thus getting loss on validation set.\n",
    "\t\t\treg = l2(w_trained, y_val)\n",
    "\t\t\tloss = train_valid_test(x_val, w_trained, b_trained, y_val, reg, reg_bool)\n",
    "\t\t\tprint(\"train/validation loss: \", loss)\n",
    "\n",
    "\t\t\tif loss < best_loss:\n",
    "\t\t\t\tbest_loss = loss\n",
    "\t\t\t\tbest_learning_rate = learning_rate\n",
    "\t\t\t\tbest_num_of_epoch = num_of_epochs\n",
    "\t\t\t\tbest_size_of_batch = size_of_batch\n",
    "\t\t\t\tbest_alpha = alpha\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tprint(\"you picked an incorrect the_set value\")\n",
    "\n",
    "\treg = l2(w_trained, y_te)\n",
    "\tloss = train_valid_test(x_te, w_trained, b_trained, y_te, reg, reg_bool)\n",
    "\tprint(\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "\tprint(\"train/validation/test loss: \", loss)\n",
    "\tprint(\"reg_bool: \", reg_bool)\n",
    "\tprint(\"best_loss: \", best_loss)\n",
    "\tprint(\"best_learning_rate: \", best_learning_rate)\n",
    "\tprint(\"best_num_of_epoch: \", best_num_of_epoch)\n",
    "\tprint(\"best_size_of_batch: \", best_size_of_batch)\n",
    "\tprint(\"best_alpha: \", best_alpha)\n",
    "\tprint(\"--------------------------------------------------------------------------------------------------------------------------------------------------------- \\n\")\n",
    "\n",
    "\treturn w_trained, b_trained\n",
    "\n",
    "\n",
    "def main():\n",
    "\t# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "\t# print(\"global based SGD using regularization\")\n",
    "\t# w_global, b_global = train_age_regressor(ttv_val = 1, the_set = 1, reg_bool = True)\n",
    "\t# print(\"batch based SGD using regularization\")\n",
    "\t# w_batch, b_batch = train_age_regressor(ttv_val = 1, the_set = 1, reg_bool = True)\n",
    "\n",
    "\tprint(\"global based SGD\")\n",
    "\tw_global, b_global = train_age_regressor(ttv_val = 1, the_set = 1, reg_bool = False)\n",
    "\tprint(\"batch based SGD\")\n",
    "\tw_batch, b_batch = train_age_regressor(ttv_val = 1, the_set = 1, reg_bool = False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1\n"
     ]
    }
   ],
   "source": [
    "# Problem 4c\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def update_b_mae(X, y, w, b, learning_rate):\n",
    "  \"\"\"\n",
    "  Performs a gradient descent step to update the bias term b for MAE loss.\n",
    "\n",
    "  Args:\n",
    "    X: The design matrix (n x d).\n",
    "    y: The response vector (n x 1).\n",
    "    w: The current weight vector (d x 1).\n",
    "    b: The current bias term (1 x 1).\n",
    "    learning_rate: The learning rate.\n",
    "\n",
    "  Returns:\n",
    "    The updated bias term b.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate the predictions.\n",
    "  predictions = X.dot(w) + b\n",
    "\n",
    "  # Calculate the signs of the errors.\n",
    "  errors = np.sign(predictions - y)\n",
    "\n",
    "  # Update the bias term.\n",
    "  b -= learning_rate * np.mean(errors)\n",
    "\n",
    "  return b\n",
    "\n",
    "# Test function with values for X, y, w, and b\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([3, 7, 11])\n",
    "w = np.array([1, 2])\n",
    "b = 0\n",
    "# Update the bias term.\n",
    "b_new = update_b_mae(X, y, w, b, learning_rate=0.1)\n",
    "print(b_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
