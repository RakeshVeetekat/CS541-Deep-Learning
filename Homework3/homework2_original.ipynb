{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 Output:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'age_regression_Xtr.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m \tw_output, b_output \u001b[38;5;241m=\u001b[39m train_age_regressor()\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m \tmain()\n",
      "Cell \u001b[1;32mIn[1], line 124\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    123\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem 3 Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 124\u001b[0m \tw_output, b_output \u001b[38;5;241m=\u001b[39m train_age_regressor()\n",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m, in \u001b[0;36mtrain_age_regressor\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_age_regressor\u001b[39m():\n\u001b[0;32m     75\u001b[0m \t\n\u001b[0;32m     76\u001b[0m \t\u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \tstarting_x_tr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_regression_Xtr.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m48\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m48\u001b[39m))\n\u001b[0;32m     78\u001b[0m \tstarting_y_tr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_regression_ytr.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \tx_te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_regression_Xte.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m48\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m48\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\rakes\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'age_regression_Xtr.npy'"
     ]
    }
   ],
   "source": [
    "# Problem 3\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate MSE given y_pred and y\n",
    "def MSE(y_pred, y):\n",
    "\ta = y.shape[0]\n",
    "\treturn (1/(2 * a)) * np.sum(np.square(y_pred - y))\n",
    "\n",
    "# Calculate the MSE given the prediction parameters and actual y value\n",
    "def calculate_error(x, w, b, y):\n",
    "\ty_pred = x.dot(w) + b\n",
    "\treturn MSE(y_pred, y)\n",
    "\n",
    "# Loop through the samples based on the batch_size hyperparameter\n",
    "def batch_loop(x, y, size):\n",
    "\tif(len(x) == len(y)):\n",
    "\t\trandom_x = x[np.random.permutation(x.shape[0])]\n",
    "\t\trandom_y = y[np.random.permutation(y.shape[0])]\n",
    "\t\tfor i in np.arange(0, y.shape[0], size):\n",
    "\t\t\tyield random_x[i:i + size], random_y[i:i + size]\n",
    "\n",
    "# Perform gradient_descent using the prediction parameters, actual y, learning rate, and regularized term\n",
    "def gradient_descent(x, y, b, w, learn_rate, alpha = 0.1):\n",
    "\ta = y.shape[0]\n",
    "\ty_pred = x.dot(w) + b\n",
    "\n",
    "\t# Store the derivatives of w and b\n",
    "\td_of_w = (1/a) * x.T.dot(y_pred - y)\n",
    "\td_of_b = (1/a) * np.sum(y_pred - y)\n",
    "\t# Add the regularized term to the derivative of w\n",
    "\td_of_w += ((alpha/a) * w)\n",
    "\t# Update parameters with gradient descent\n",
    "\tw = w - (learn_rate * d_of_w)\n",
    "\tb = b - (learn_rate * d_of_b)\n",
    "\n",
    "\treturn w, b\n",
    "\n",
    "# Perform SGD\n",
    "def stochastic_gradient_descent(x, y, b, w, learn_rate: float, num_of_epoch: int, batch_size: int, dataset_size: int, alpha):\n",
    "\t\n",
    "\tfor n in range(num_of_epoch - 1):\n",
    "\t\tfor mini_batch_x, mini_batch_y in batch_loop(x, y, batch_size):\n",
    "\t\t\tw, b = gradient_descent(mini_batch_x, mini_batch_y, b, w, learn_rate, alpha)\n",
    "\t\t\n",
    "\treturn w, b \n",
    "\n",
    "# Find the lowest error by performing SGD\n",
    "def find_lowest_error(x, y, learn_rate, num_of_epoch, batch_size, alpha):\n",
    "\tdataset_size = len(y)\n",
    "\tm = np.expand_dims(a=y, axis=-1)\n",
    "\tw = np.random.rand(x.shape[1]) * np.sqrt(1/(x.shape[1] + m.shape[1]))\n",
    "\tb = np.random.rand(m.shape[1])\n",
    "\tw_trained, b_trained = stochastic_gradient_descent(x, y, b, w, learn_rate, num_of_epoch, batch_size, dataset_size, alpha)\n",
    "\treturn w_trained, b_trained\n",
    "\n",
    "# Implementation of grid_search to tune our hyperparameters by looping through the various values we have for each\n",
    "def grid_search():\n",
    "\thyperparameters = {\n",
    "\t\t\"learn_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "\t\t\"num_of_epoch\": [5, 10, 15, 20],\n",
    "\t\t\"batch_size\": [10, 20, 50, 100],\n",
    "\t\t\"alpha\": [0.75, 0.5, 0.25, 0.1]\t\n",
    "\t}\n",
    "\tfor a in range(len(hyperparameters[\"num_of_epoch\"])):\n",
    "\t\tfor b in range(len(hyperparameters[\"batch_size\"])):\n",
    "\t\t\tfor c in range(len(hyperparameters[\"learn_rate\"])):\n",
    "\t\t\t\tfor d in range(len(hyperparameters[\"alpha\"])):\n",
    "\t\t\t\t\t\tyield hyperparameters[\"num_of_epoch\"][a], hyperparameters[\"batch_size\"][b], hyperparameters[\"learn_rate\"][c], hyperparameters[\"alpha\"][d]\n",
    "\n",
    "# This function will train the age regressor based on various hyperparameters provided in the grid_search() function\n",
    "def train_age_regressor():\n",
    "\t\n",
    "\t# Load data\n",
    "\tstarting_x_tr = np.reshape(np.load(\"age_regression_Xtr.npy\"), (-1, 48*48))\n",
    "\tstarting_y_tr = np.load(\"age_regression_ytr.npy\")\n",
    "\tx_te = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))\n",
    "\ty_te = np.load(\"age_regression_yte.npy\")\n",
    "\tx_tr, x_val, y_tr, y_val = train_test_split(starting_x_tr, starting_y_tr, train_size=0.8)\n",
    "\n",
    "\t# Initialize best hyperparameter values as the worst they could be\n",
    "\tbest_error = 1000000\n",
    "\tbest_num_of_epoch = -1\n",
    "\tbest_batch_size = -1\n",
    "\tbest_learn_rate = -1\n",
    "\tbest_alpha = -1\n",
    "\n",
    "\t# Loop through each combination of the hyperparameters in grid_search() to find the best combination to minimize MSE\n",
    "\tfor num_of_epoch, batch_size, learn_rate, alpha in grid_search():\n",
    "\n",
    "\t\tw_trained, b_trained = find_lowest_error(x_tr, y_tr, learn_rate, num_of_epoch, batch_size, alpha)\n",
    "\n",
    "\t\t# Calculate the MSE from the validation dataset\n",
    "\t\terror = calculate_error(x_val, w_trained, b_trained, y_val)\n",
    "\t\tprint(\"train/validation unregularized MSE: \", error)\n",
    "\t\tprint(x_val.shape)\n",
    "\n",
    "\t\t# Store the hyperparameters that led to reduced error in the following variables\n",
    "\t\tif error < best_error:\n",
    "\t\t\tbest_error = error\n",
    "\t\t\tbest_learn_rate = learn_rate\n",
    "\t\t\tbest_num_of_epoch = num_of_epoch\n",
    "\t\t\tbest_batch_size = batch_size\n",
    "\t\t\tbest_alpha = alpha\n",
    "\n",
    "\t# Finally, calculate the error using the trained weights and biases\n",
    "\terror = calculate_error(x_te, w_trained, b_trained, y_te)\n",
    "\tprint(\"\\n\")\n",
    "\tprint(\"Results of training:\")\n",
    "\tprint(\"best error from validation dataset: \", best_error)\n",
    "\tprint(\"best learning rate: \", best_learn_rate)\n",
    "\tprint(\"best number of epochs: \", best_num_of_epoch)\n",
    "\tprint(\"best batch size: \", best_batch_size)\n",
    "\tprint(\"best reg term: \", best_alpha)\n",
    "\tprint(\"unregularized MSE from test dataset: \", error)\n",
    "\n",
    "\treturn w_trained, b_trained\n",
    "\n",
    "def main():\n",
    "\n",
    "\tprint(\"Problem 3 Output:\")\n",
    "\tw_output, b_output = train_age_regressor()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
