{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "# For this assignment, assume that every hidden layer has the same number of neurons.\n",
    "NUM_HIDDEN_LAYERS = 3\n",
    "NUM_INPUT = 784\n",
    "NUM_HIDDEN = 10\n",
    "NUM_OUTPUT = 10\n",
    "\n",
    "# Unpack a list of weights and biases into their individual np.arrays.\n",
    "def unpack (weightsAndBiases):\n",
    "    # Unpack arguments\n",
    "    Ws = []\n",
    "\n",
    "    # Weight matrices\n",
    "    start = 0\n",
    "    end = NUM_INPUT*NUM_HIDDEN\n",
    "    W = weightsAndBiases[start:end]\n",
    "    Ws.append(W)\n",
    "\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "        start = end\n",
    "        end = end + NUM_HIDDEN*NUM_HIDDEN\n",
    "        W = weightsAndBiases[start:end]\n",
    "        Ws.append(W)\n",
    "\n",
    "    start = end\n",
    "    end = end + NUM_HIDDEN*NUM_OUTPUT\n",
    "    W = weightsAndBiases[start:end]\n",
    "    Ws.append(W)\n",
    "\n",
    "    Ws[0] = Ws[0].reshape(NUM_HIDDEN, NUM_INPUT)\n",
    "    for i in range(1, NUM_HIDDEN_LAYERS):\n",
    "        # Convert from vectors into matrices\n",
    "        Ws[i] = Ws[i].reshape(NUM_HIDDEN, NUM_HIDDEN)\n",
    "    Ws[-1] = Ws[-1].reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
    "\n",
    "    # Bias terms\n",
    "    bs = []\n",
    "    start = end\n",
    "    end = end + NUM_HIDDEN\n",
    "    b = weightsAndBiases[start:end]\n",
    "    bs.append(b)\n",
    "\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "        start = end\n",
    "        end = end + NUM_HIDDEN\n",
    "        b = weightsAndBiases[start:end]\n",
    "        bs.append(b)\n",
    "\n",
    "    start = end\n",
    "    end = end + NUM_OUTPUT\n",
    "    b = weightsAndBiases[start:end]\n",
    "    bs.append(b)\n",
    "\n",
    "    return Ws, bs\n",
    "\n",
    "def forward_prop (x, y, weightsAndBiases):\n",
    "    Ws, bs = unpack(weightsAndBiases)\n",
    "\n",
    "    # Return loss, pre-activations, post-activations, and predictions\n",
    "    return loss, zs, hs, yhat\n",
    "   \n",
    "def back_prop (x, y, weightsAndBiases):\n",
    "    loss, zs, hs, yhat = forward_prop(x, y, weightsAndBiases)\n",
    "\n",
    "    dJdWs = []  # Gradients w.r.t. weights\n",
    "    dJdbs = []  # Gradients w.r.t. biases\n",
    "\n",
    "    # TODO\n",
    "    for i in range(NUM_HIDDEN_LAYERS, -1, -1):\n",
    "        pass\n",
    "        # TODO\n",
    "        \n",
    "    # Concatenate gradients\n",
    "    return np.hstack([ dJdW.flatten() for dJdW in dJdWs ] + [ dJdb.flatten() for dJdb in dJdbs ]) \n",
    "\n",
    "def train (trainX, trainY, weightsAndBiases, testX, testY):\n",
    "    NUM_EPOCHS = 100\n",
    "    trajectory = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # TODO: implement SGD.\n",
    "        # TODO: save the current set of weights and biases into trajectory; this is\n",
    "        # useful for visualizing the SGD trajectory.\n",
    "        \n",
    "    return weightsAndBiases, trajectory\n",
    "\n",
    "# Performs a standard form of random initialization of weights and biases\n",
    "def initWeightsAndBiases ():\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    np.random.seed(0)\n",
    "    W = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    Ws.append(W)\n",
    "    b = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    bs.append(b)\n",
    "\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "        W = 2*(np.random.random(size=(NUM_HIDDEN, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "        Ws.append(W)\n",
    "        b = 0.01 * np.ones(NUM_HIDDEN)\n",
    "        bs.append(b)\n",
    "\n",
    "    W = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    Ws.append(W)\n",
    "    b = 0.01 * np.ones(NUM_OUTPUT)\n",
    "    bs.append(b)\n",
    "    return np.hstack([ W.flatten() for W in Ws ] + [ b.flatten() for b in bs ])\n",
    "\n",
    "def plotSGDPath (trainX, trainY, trajectory):\n",
    "    # TODO: change this toy plot to show a 2-d projection of the weight space\n",
    "    # along with the associated loss (cross-entropy), plus a superimposed \n",
    "    # trajectory across the landscape that was traversed using SGD. Use\n",
    "    # sklearn.decomposition.PCA's fit_transform and inverse_transform methods.\n",
    "\n",
    "    def toyFunction (x1, x2):\n",
    "        return np.sin((2 * x1**2 - x2) / 10.)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    # Compute the CE loss on a grid of points (corresonding to different w).\n",
    "    axis1 = np.arange(-np.pi, +np.pi, 0.05)  # Just an example\n",
    "    axis2 = np.arange(-np.pi, +np.pi, 0.05)  # Just an example\n",
    "    Xaxis, Yaxis = np.meshgrid(axis1, axis2)\n",
    "    Zaxis = np.zeros((len(axis1), len(axis2)))\n",
    "    for i in range(len(axis1)):\n",
    "        for j in range(len(axis2)):\n",
    "            Zaxis[i,j] = toyFunction(Xaxis[i,j], Yaxis[i,j])\n",
    "    ax.plot_surface(Xaxis, Yaxis, Zaxis, alpha=0.6)  # Keep alpha < 1 so we can see the scatter plot too.\n",
    "\n",
    "    # Now superimpose a scatter plot showing the weights during SGD.\n",
    "    Xaxis = 2*np.pi*np.random.random(8) - np.pi  # Just an example\n",
    "    Yaxis = 2*np.pi*np.random.random(8) - np.pi  # Just an example\n",
    "    Zaxis = toyFunction(Xaxis, Yaxis)\n",
    "    ax.scatter(Xaxis, Yaxis, Zaxis, color='r')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: Load data and split into train, validation, test sets\n",
    "    # trainX = ...\n",
    "    # trainY = ...\n",
    "    # ...\n",
    "\n",
    "    # Initialize weights and biases randomly\n",
    "    weightsAndBiases = initWeightsAndBiases()\n",
    "\n",
    "    # Perform gradient check on 5 training examples\n",
    "    print(scipy.optimize.check_grad(lambda wab: forward_prop(np.atleast_2d(trainX[:,0:5]), np.atleast_2d(trainY[:,0:5]), wab)[0], \\\n",
    "                                    lambda wab: back_prop(np.atleast_2d(trainX[:,0:5]), np.atleast_2d(trainY[:,0:5]), wab), \\\n",
    "                                    weightsAndBiases))\n",
    "\n",
    "    weightsAndBiases, trajectory = train(trainX, trainY, weightsAndBiases, testX, testY)\n",
    "    \n",
    "    # Plot the SGD trajectory\n",
    "    plotSGDPath(trainX, trainY, ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.00348668,  0.01537067,  0.00734024, ..., -0.00522112,\n",
      "         0.02448963,  0.02271666],\n",
      "       [-0.02839902, -0.02454405, -0.01398581, ..., -0.01142138,\n",
      "        -0.03081451, -0.01936374],\n",
      "       [-0.010144  , -0.00463272,  0.00649477, ..., -0.03564977,\n",
      "        -0.03072755, -0.01953634],\n",
      "       ...,\n",
      "       [-0.00888622, -0.03211392,  0.02086261, ...,  0.021809  ,\n",
      "         0.02075498,  0.02754609],\n",
      "       [-0.03208987,  0.03444406, -0.00525275, ..., -0.00877496,\n",
      "        -0.03446212, -0.01792557],\n",
      "       [ 0.01078592, -0.02691873,  0.01152538, ..., -0.0112461 ,\n",
      "         0.02852671, -0.02505609]]), array([[-0.00623292, -0.24997897, -0.21974869,  0.05621852, -0.06571117,\n",
      "        -0.04826134,  0.00514278,  0.10165326, -0.00042408, -0.10007193],\n",
      "       [-0.19428498, -0.02246916, -0.07870718,  0.19950509, -0.30655298,\n",
      "         0.04452347,  0.07492022,  0.30857287,  0.24372573,  0.01111052],\n",
      "       [-0.13375457, -0.30978621, -0.16394463,  0.21973743,  0.29931353,\n",
      "         0.12715787,  0.06568317,  0.06718836,  0.09480991, -0.31509623],\n",
      "       [-0.08224553, -0.10797355,  0.30227149, -0.31576816,  0.14947752,\n",
      "        -0.05990182, -0.27526971, -0.08794113, -0.23536908, -0.29259925],\n",
      "       [ 0.1160259 , -0.30061587, -0.308661  , -0.04681181, -0.23365099,\n",
      "        -0.03407189, -0.08472734, -0.25154189,  0.08835858,  0.09469262],\n",
      "       [ 0.09150234,  0.23061227, -0.31194368, -0.2986866 ,  0.01465101,\n",
      "        -0.02149367,  0.31026211, -0.0901021 , -0.07013667,  0.21231935],\n",
      "       [ 0.07785033, -0.27127591, -0.27976083,  0.01805775, -0.0652869 ,\n",
      "         0.14889985,  0.08099364, -0.31603389, -0.3055534 ,  0.21485431],\n",
      "       [-0.21213496,  0.04401607,  0.27859464,  0.29510795,  0.21772455,\n",
      "         0.24597666,  0.25618165, -0.26780962,  0.2259402 , -0.05329568],\n",
      "       [-0.28924364,  0.02097925, -0.24241765, -0.26619138, -0.02050496,\n",
      "        -0.11715038,  0.24116648, -0.11663059,  0.20401517, -0.31313429],\n",
      "       [ 0.28410442, -0.28186707, -0.14996573, -0.30048812,  0.29052157,\n",
      "        -0.28353923, -0.09549886,  0.1036217 , -0.11256709, -0.31149274]]), array([[-0.16407634,  0.13578355, -0.07698348,  0.20256623,  0.1511442 ,\n",
      "         0.05438268, -0.25647632,  0.30484319,  0.01307466, -0.15167594],\n",
      "       [ 0.06459562,  0.13077904, -0.02055196,  0.08970179, -0.2478972 ,\n",
      "         0.23657983,  0.11430061,  0.17265211, -0.2442954 ,  0.01220809],\n",
      "       [ 0.12180763,  0.01409291, -0.25348249, -0.27786352,  0.07337511,\n",
      "         0.18777052,  0.31409578,  0.04314452, -0.1710809 , -0.06003485],\n",
      "       [-0.24878293, -0.05447297,  0.21514068, -0.0694438 , -0.28350858,\n",
      "        -0.21129617,  0.1073136 ,  0.12094163, -0.19097517,  0.19615073],\n",
      "       [-0.13859907,  0.03523741, -0.12161763,  0.12213793, -0.07862996,\n",
      "         0.16558124, -0.01048226,  0.08871469,  0.26945966, -0.04089155],\n",
      "       [ 0.03203069, -0.19661917,  0.1223373 , -0.18169976,  0.12014012,\n",
      "         0.2676447 ,  0.27910319,  0.19089291,  0.02475722,  0.21327572],\n",
      "       [-0.02671511, -0.07784408,  0.12796797, -0.18510449, -0.26924926,\n",
      "        -0.0847322 , -0.10079106, -0.24000163,  0.22424489,  0.106346  ],\n",
      "       [-0.299916  ,  0.02539913, -0.21895409,  0.00709624,  0.17155518,\n",
      "        -0.05498972,  0.06126273,  0.02937056,  0.04908226, -0.01383994],\n",
      "       [-0.21058748, -0.16341748,  0.24921869,  0.15065501,  0.30539117,\n",
      "        -0.2819929 , -0.11600999,  0.29047904, -0.03834021,  0.0659888 ],\n",
      "       [ 0.04410836, -0.22724797,  0.14616319, -0.27934764,  0.02400984,\n",
      "         0.03329106, -0.1885676 ,  0.15838219,  0.25056422,  0.30298921]]), array([[-0.29926725, -0.12034054,  0.26868217, -0.28428534, -0.14418393,\n",
      "         0.02111678,  0.15191401, -0.17199222, -0.12627342, -0.19319086],\n",
      "       [-0.03601072,  0.00274868,  0.25249331,  0.17124011,  0.13619617,\n",
      "         0.04596039, -0.1084154 , -0.27190722, -0.15254346, -0.19309673],\n",
      "       [-0.17209287, -0.17280274, -0.04392614, -0.07544253, -0.10306068,\n",
      "         0.24111581, -0.18800537, -0.2127241 , -0.26815931,  0.26870516],\n",
      "       [-0.19331237,  0.04570629, -0.07986786,  0.03497463,  0.09016687,\n",
      "         0.2150836 ,  0.00459355, -0.10926257, -0.20170277,  0.06755702],\n",
      "       [-0.23274086,  0.2250322 , -0.29865215,  0.2183868 , -0.17756403,\n",
      "        -0.00767937,  0.14579409,  0.17972289, -0.06211992, -0.00034484],\n",
      "       [-0.29752437, -0.09864658, -0.15079078, -0.06952642, -0.12592609,\n",
      "        -0.21519944,  0.27937478, -0.04472799,  0.08715409,  0.26990744],\n",
      "       [ 0.04583647, -0.23590644,  0.18251488,  0.17371281, -0.0603301 ,\n",
      "         0.02491472, -0.04750626, -0.17440312,  0.16628629,  0.15459428],\n",
      "       [ 0.09300631, -0.02805542, -0.02225862, -0.1274753 , -0.14816878,\n",
      "         0.22070889,  0.22728822, -0.30652281,  0.13889125, -0.1075667 ],\n",
      "       [ 0.26521855, -0.16309754, -0.1041002 ,  0.15642908, -0.26936322,\n",
      "         0.13157545,  0.11047659, -0.12394759,  0.13456713,  0.25483196],\n",
      "       [ 0.19222931,  0.01128449, -0.23246188,  0.24813712, -0.00650286,\n",
      "        -0.05826337,  0.03020176, -0.18116839, -0.10360181,  0.14894678]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "# For this assignment, assume that every hidden layer has the same number of neurons.\n",
    "NUM_HIDDEN_LAYERS = 3\n",
    "NUM_INPUT = 784\n",
    "NUM_HIDDEN = 10\n",
    "NUM_OUTPUT = 10\n",
    "\n",
    "# Performs a standard form of random initialization of weights and biases\n",
    "def initWeightsAndBiases ():\n",
    "    Ws = []\n",
    "    bs = []\n",
    "\n",
    "    np.random.seed(0)\n",
    "    W = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    Ws.append(W)\n",
    "    b = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    bs.append(b)\n",
    "\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "        W = 2*(np.random.random(size=(NUM_HIDDEN, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "        Ws.append(W)\n",
    "        b = 0.01 * np.ones(NUM_HIDDEN)\n",
    "        bs.append(b)\n",
    "\n",
    "    W = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    Ws.append(W)\n",
    "    b = 0.01 * np.ones(NUM_OUTPUT)\n",
    "    bs.append(b)\n",
    "    return np.hstack([ W.flatten() for W in Ws ] + [ b.flatten() for b in bs ])\n",
    "\n",
    "# Unpack a list of weights and biases into their individual np.arrays.\n",
    "def unpack (weightsAndBiases):\n",
    "    # Unpack arguments\n",
    "    Ws = []\n",
    "\n",
    "    # Weight matrices\n",
    "    start = 0\n",
    "    end = NUM_INPUT*NUM_HIDDEN\n",
    "    W = weightsAndBiases[start:end]\n",
    "    Ws.append(W)\n",
    "\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "        start = end\n",
    "        end = end + NUM_HIDDEN*NUM_HIDDEN\n",
    "        W = weightsAndBiases[start:end]\n",
    "        Ws.append(W)\n",
    "\n",
    "    start = end\n",
    "    end = end + NUM_HIDDEN*NUM_OUTPUT\n",
    "    W = weightsAndBiases[start:end]\n",
    "    Ws.append(W)\n",
    "\n",
    "    Ws[0] = Ws[0].reshape(NUM_HIDDEN, NUM_INPUT)\n",
    "    for i in range(1, NUM_HIDDEN_LAYERS):\n",
    "        # Convert from vectors into matrices\n",
    "        Ws[i] = Ws[i].reshape(NUM_HIDDEN, NUM_HIDDEN)\n",
    "    Ws[-1] = Ws[-1].reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
    "\n",
    "    # Bias terms\n",
    "    bs = []\n",
    "    start = end\n",
    "    end = end + NUM_HIDDEN\n",
    "    b = weightsAndBiases[start:end]\n",
    "    bs.append(b)\n",
    "\n",
    "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
    "        start = end\n",
    "        end = end + NUM_HIDDEN\n",
    "        b = weightsAndBiases[start:end]\n",
    "        bs.append(b)\n",
    "\n",
    "    start = end\n",
    "    end = end + NUM_OUTPUT\n",
    "    b = weightsAndBiases[start:end]\n",
    "    bs.append(b)\n",
    "\n",
    "    return Ws, bs\n",
    "\n",
    "def plotSGDPath (trainX, trainY, trajectory):\n",
    "    # TODO: change this toy plot to show a 2-d projection of the weight space\n",
    "    # along with the associated loss (cross-entropy), plus a superimposed \n",
    "    # trajectory across the landscape that was traversed using SGD. Use\n",
    "    # sklearn.decomposition.PCA's fit_transform and inverse_transform methods.\n",
    "\n",
    "    def toyFunction (x1, x2):\n",
    "        return np.sin((2 * x1**2 - x2) / 10.)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    # Compute the CE loss on a grid of points (corresonding to different w).\n",
    "    axis1 = np.arange(-np.pi, +np.pi, 0.05)  # Just an example\n",
    "    axis2 = np.arange(-np.pi, +np.pi, 0.05)  # Just an example\n",
    "    Xaxis, Yaxis = np.meshgrid(axis1, axis2)\n",
    "    Zaxis = np.zeros((len(axis1), len(axis2)))\n",
    "    for i in range(len(axis1)):\n",
    "        for j in range(len(axis2)):\n",
    "            Zaxis[i,j] = toyFunction(Xaxis[i,j], Yaxis[i,j])\n",
    "    ax.plot_surface(Xaxis, Yaxis, Zaxis, alpha=0.6)  # Keep alpha < 1 so we can see the scatter plot too.\n",
    "\n",
    "    # Now superimpose a scatter plot showing the weights during SGD.\n",
    "    Xaxis = 2*np.pi*np.random.random(8) - np.pi  # Just an example\n",
    "    Yaxis = 2*np.pi*np.random.random(8) - np.pi  # Just an example\n",
    "    Zaxis = toyFunction(Xaxis, Yaxis)\n",
    "    ax.scatter(Xaxis, Yaxis, Zaxis, color='r')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "weightsAndBiases = initWeightsAndBiases()\n",
    "ws, bs = unpack(weightsAndBiases)\n",
    "print(ws)\n",
    "\n",
    "# Plot the SGD trajectory\n",
    "plotSGDPath(trainX, trainY, ws)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
